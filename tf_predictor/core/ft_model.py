"""
Core FT-Transformer implementation.

­ЪДа FT-TRANSFORMER CLS MODEL (UNIFIED TOKEN ARCHITECTURE)
=========================================================

The FT-Transformer CLS Model is a unified architecture for time series forecasting that
processes STATIC categorical features and TIME-VARYING numerical sequences together through
a single transformer. Unlike dual-path approaches, all features are tokenized and attend
to each other in a shared attention mechanism.

­ЪЊі ARCHITECTURE OVERVIEW:
РћїРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћљ
Рћѓ  STATIC Categorical + TIME-VARYING Numerical Рєњ Unified Tokenization Рєњ       Рћѓ
Рћѓ  [batch, num_cat] + [batch, seq, num_num]   Рєњ [CLS, num_tokens, cat_tokens]Рћѓ
Рћѓ                                               РєЊ                              Рћѓ
Рћѓ                                        Shared Transformer                    Рћѓ
Рћѓ                                               РєЊ                              Рћѓ
Рћѓ                                          CLS Token Рєњ Prediction              Рћѓ
РћћРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћў

­Ъћё DATA FLOW WITH MATRIX DIMENSIONS:

Example Configuration:
- batch_size = 32
- sequence_length = 10
- num_numerical = 8 (price, volume, technical indicators, etc.)
- num_categorical = 2 (symbol, sector)
- cat_cardinalities = [100, 5] (100 stock symbols, 5 sectors)
- d_model = 128 (embedding dimension)
- num_heads = 4
- num_layers = 3
- output_dim = 1 (single-step forecast)

­ЪЈЌ№ИЈ UNIFIED TOKENIZATION & PROCESSING:

РћїРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћљ
Рћѓ STEP 1: INPUT FORMAT                                                        Рћѓ
Рћѓ РћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђ Рћѓ
Рћѓ                                                                              Рћѓ
Рћѓ Numerical Sequences (Time-Varying):                                         Рћѓ
Рћѓ   x_num: [32, 10, 8]  # [batch_size, sequence_length, num_numerical]      Рћѓ
Рћѓ   Represents historical price data across 10 timesteps:                     Рћѓ
Рћѓ     - open, high, low, close, volume                                        Рћѓ
Рћѓ     - technical indicators (RSI, MACD, SMA)                                 Рћѓ
Рћѓ                                                                              Рћѓ
Рћѓ Categorical Features (Static):                                              Рћѓ
Рћѓ   x_cat: [32, 2]  # [batch_size, num_categorical]                          Рћѓ
Рћѓ   Values are integer indices (label encoded):                               Рћѓ
Рћѓ     - Column 0: symbol indices [0-99]                                       Рћѓ
Рћѓ     - Column 1: sector indices [0-4]                                        Рћѓ
РћћРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћў

РћїРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћљ
Рћѓ STEP 2: NUMERICAL TOKENIZATION (TIME-VARYING FEATURES)                      Рћѓ
Рћѓ РћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђ Рћѓ
Рћѓ                                                                              Рћѓ
Рћѓ For each timestep t and each numerical feature j:                           Рћѓ
Рћѓ   token_tj = x_num[:, t, j] * W_j + b_j                                    Рћѓ
Рћѓ                                                                              Рћѓ
Рћѓ NumericalTokenizer: Creates per-feature linear transformations              Рћѓ
Рћѓ   W_j: [1, d_model] for each numerical feature j                           Рћѓ
Рћѓ   b_j: [d_model] for each numerical feature j                              Рћѓ
Рћѓ                                                                              Рћѓ
Рћѓ Process each timestep:                                                       Рћѓ
Рћѓ   for t in range(10):                                                        Рћѓ
Рћѓ     x_num_t = x_num[:, t, :]  # [32, 8]                                    Рћѓ
Рћѓ     tokens_t = NumericalTokenizer(x_num_t)  # [32, 8, 128]                 Рћѓ
Рћѓ     tokens_t += temporal_pos_encoding[:, t, :]  # Add temporal info        Рћѓ
Рћѓ                                                                              Рћѓ
Рћѓ Stack all timesteps:                                                         Рћѓ
Рћѓ   num_tokens_list = [tokens_0, tokens_1, ..., tokens_9]                    Рћѓ
Рћѓ   num_tokens = cat(num_tokens_list, dim=1)                                 Рћѓ
Рћѓ   Рєњ  [32, 80, 128]  # [batch, seq_len * num_numerical, d_model]           Рћѓ
Рћѓ                      # 10 timesteps ├Ќ 8 features = 80 tokens               Рћѓ
РћћРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћў

РћїРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћљ
Рћѓ STEP 3: CATEGORICAL TOKENIZATION (STATIC FEATURES)                          Рћѓ
Рћѓ РћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђ Рћѓ
Рћѓ                                                                              Рћѓ
Рћѓ Logarithmic Embedding Dimension Scaling:                                    Рћѓ
Рћѓ   FORMULA: emb_dim = int(8 * log2(cardinality + 1))                        Рћѓ
Рћѓ            emb_dim = clamp(emb_dim, d_model/4, d_model)                    Рћѓ
Рћѓ                                                                              Рћѓ
Рћѓ For our example:                                                             Рћѓ
Рћѓ   symbol: cardinality=100 Рєњ emb_dim = int(8*log2(101)) РЅѕ 53               Рћѓ
Рћѓ   sector: cardinality=5   Рєњ emb_dim = int(8*log2(6)) РЅѕ 20 Рєњ clamped to 32 Рћѓ
Рћѓ                                                                              Рћѓ
Рћѓ Symbol Processing:                                                           Рћѓ
Рћѓ   symbol_indices = x_cat[:, 0]  # [32]                                     Рћѓ
Рћѓ   symbol_emb = Embedding(100, 53)(symbol_indices)  # [32, 53]              Рћѓ
Рћѓ   symbol_proj = Linear(53, 128)(symbol_emb)        # [32, 128]             Рћѓ
Рћѓ                                                                              Рћѓ
Рћѓ Sector Processing:                                                           Рћѓ
Рћѓ   sector_indices = x_cat[:, 1]  # [32]                                     Рћѓ
Рћѓ   sector_emb = Embedding(5, 32)(sector_indices)    # [32, 32]              Рћѓ
Рћѓ   sector_proj = Linear(32, 128)(sector_emb)        # [32, 128]             Рћѓ
Рћѓ                                                                              Рћѓ
Рћѓ Stack categorical tokens:                                                    Рћѓ
Рћѓ   cat_tokens_list = [symbol_proj, sector_proj]                             Рћѓ
Рћѓ   cat_tokens = stack(cat_tokens_list, dim=1)                               Рћѓ
Рћѓ   Рєњ  [32, 2, 128]  # [batch, num_categorical, d_model]                     Рћѓ
РћћРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћў

РћїРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћљ
Рћѓ STEP 4: TOKEN ASSEMBLY WITH CLS                                             Рћѓ
Рћѓ РћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђ Рћѓ
Рћѓ                                                                              Рћѓ
Рћѓ Token Ordering: [CLS, numerical_tokens, categorical_tokens]                 Рћѓ
Рћѓ                                                                              Рћѓ
Рћѓ 1. CLS Token (learnable aggregation token):                                 Рћѓ
Рћѓ      cls_token = CLSToken(d_model)              # [1, 1, 128]              Рћѓ
Рћѓ      cls_expanded = cls_token.expand(32, -1, -1) # [32, 1, 128]            Рћѓ
Рћѓ                                                                              Рћѓ
Рћѓ 2. Numerical Tokens (time-varying):                                         Рћѓ
Рћѓ      num_tokens: [32, 80, 128]  # 10 timesteps ├Ќ 8 features                Рћѓ
Рћѓ                                                                              Рћѓ
Рћѓ 3. Categorical Tokens (static):                                             Рћѓ
Рћѓ      cat_tokens: [32, 2, 128]  # 2 categorical features                    Рћѓ
Рћѓ                                                                              Рћѓ
Рћѓ Concatenate all tokens:                                                     Рћѓ
Рћѓ   all_tokens = cat([cls_expanded, num_tokens, cat_tokens], dim=1)         Рћѓ
Рћѓ   Рєњ  [32, 83, 128]  # [batch, 1 + 80 + 2, d_model]                        Рћѓ
Рћѓ                      # 1 CLS + 80 numerical + 2 categorical = 83 tokens    Рћѓ
РћћРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћў

РћїРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћљ
Рћѓ STEP 5: UNIFIED TRANSFORMER PROCESSING                                      Рћѓ
Рћѓ РћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђ Рћѓ
Рћѓ                                                                              Рћѓ
Рћѓ Multi-Head Self-Attention (3 layers, num_heads=4):                          Рћѓ
Рћѓ                                                                              Рћѓ
Рћѓ Layer configuration:                                                         Рћѓ
Рћѓ   d_model = 128                                                              Рћѓ
Рћѓ   num_heads = 4                                                              Рћѓ
Рћѓ   d_head = 128 / 4 = 32                                                      Рћѓ
Рћѓ   d_ffn = 4 * 128 = 512                                                      Рћѓ
Рћѓ                                                                              Рћѓ
Рћѓ For each transformer layer:                                                  Рћѓ
Рћѓ                                                                              Рћѓ
Рћѓ   A. Multi-Head Self-Attention:                                              Рћѓ
Рћѓ      Q = all_tokens @ W_q  # [32, 83, 128] @ [128, 128] Рєњ [32, 83, 128]   Рћѓ
Рћѓ      K = all_tokens @ W_k  # [32, 83, 128] @ [128, 128] Рєњ [32, 83, 128]   Рћѓ
Рћѓ      V = all_tokens @ W_v  # [32, 83, 128] @ [128, 128] Рєњ [32, 83, 128]   Рћѓ
Рћѓ                                                                              Рћѓ
Рћѓ      Reshape for multi-head attention:                                       Рћѓ
Рћѓ      Q = Q.view(32, 83, 4, 32).transpose(1, 2)  # [32, 4, 83, 32]          Рћѓ
Рћѓ      K = K.view(32, 83, 4, 32).transpose(1, 2)  # [32, 4, 83, 32]          Рћѓ
Рћѓ      V = V.view(32, 83, 4, 32).transpose(1, 2)  # [32, 4, 83, 32]          Рћѓ
Рћѓ                                                                              Рћѓ
Рћѓ      Attention computation:                                                  Рћѓ
Рћѓ      scores = Q @ K^T  # [32, 4, 83, 32] @ [32, 4, 32, 83] Рєњ [32, 4, 83, 83]Рћѓ
Рћѓ      attn_weights = softmax(scores / Рѕџ32)  # [32, 4, 83, 83]               Рћѓ
Рћѓ      attn_output = attn_weights @ V  # [32, 4, 83, 83] @ [32, 4, 83, 32]  Рћѓ
Рћѓ                                       # Рєњ [32, 4, 83, 32]                   Рћѓ
Рћѓ                                                                              Рћѓ
Рћѓ      Concatenate heads:                                                      Рћѓ
Рћѓ      attn_output = attn_output.transpose(1, 2).reshape(32, 83, 128)        Рћѓ
Рћѓ      output = LayerNorm(all_tokens + attn_output)  # [32, 83, 128]         Рћѓ
Рћѓ                                                                              Рћѓ
Рћѓ   B. Feed-Forward Network:                                                   Рћѓ
Рћѓ      ffn_hidden = Linear(128, 512)(output)     # [32, 83, 512]             Рћѓ
Рћѓ      ffn_hidden = ReLU(ffn_hidden)             # [32, 83, 512]             Рћѓ
Рћѓ      ffn_output = Linear(512, 128)(ffn_hidden) # [32, 83, 128]             Рћѓ
Рћѓ      output = LayerNorm(output + ffn_output)   # [32, 83, 128]             Рћѓ
Рћѓ                                                                              Рћѓ
Рћѓ Final transformer output: [32, 83, 128]                                      Рћѓ
Рћѓ                                                                              Рћѓ
Рћѓ KEY INSIGHT: All tokens (CLS, numerical at each timestep, categorical)      Рћѓ
Рћѓ              attend to each other, enabling rich cross-feature interactions Рћѓ
РћћРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћў

РћїРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћљ
Рћѓ STEP 6: CLS EXTRACTION & PREDICTION                                         Рћѓ
Рћѓ РћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђ Рћѓ
Рћѓ                                                                              Рћѓ
Рћѓ Extract CLS token (aggregated representation):                              Рћѓ
Рћѓ   cls_output = transformer_output[:, 0, :]  # [32, 128]                    Рћѓ
Рћѓ                                                                              Рћѓ
Рћѓ Prediction Head (MultiHorizonHead):                                         Рћѓ
Рћѓ   predictions = Linear(128, output_dim)(cls_output)                         Рћѓ
Рћѓ   Рєњ  [32, 1]  # [batch, output_dim]                                        Рћѓ
Рћѓ                                                                              Рћѓ
Рћѓ For multi-horizon forecasting (output_dim=3):                               Рћѓ
Рћѓ   predictions = Linear(128, 3)(cls_output)                                  Рћѓ
Рћѓ   Рєњ  [32, 3]  # Forecasts for t+1, t+2, t+3                                Рћѓ
РћћРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћў

­Ъј» KEY ARCHITECTURAL FEATURES:

1. Unified Tokenization:
   - All features converted to same d_model dimensional space
   - Numerical: Linear transformation per feature
   - Categorical: Embedding + projection to d_model

2. Temporal Positional Encoding:
   - Added to numerical tokens to preserve time ordering
   - Learnable: pos_emb[t] Рѕѕ РёЮ^{d_model} for each timestep t

3. Single Shared Transformer:
   - All tokens processed together in unified attention
   - CLS token attends to all numerical and categorical tokens
   - Enables cross-modality feature interactions

4. Logarithmic Embedding Scaling:
   - emb_dim = int(8 * log2(cardinality + 1))
   - Bounds: [d_model/4, d_model]
   - Matches embedding capacity to information content

­ЪДа COMPUTATIONAL COMPLEXITY:

Memory Usage (per forward pass):
- All tokens: 32 ├Ќ 83 ├Ќ 128 ├Ќ 4 bytes РЅѕ 1.36 MB
- Attention matrix: 32 ├Ќ 4 ├Ќ 83┬▓ ├Ќ 4 bytes РЅѕ 3.5 MB (per layer)
- Total: ~1.36 MB + ~10.5 MB (3 layers) РЅѕ 11.86 MB

Time Complexity:
- Tokenization: O(sequence_length ├Ќ num_numerical ├Ќ d_model)
- Self-Attention: O(num_layers ├Ќ num_tokens┬▓ ├Ќ d_model)
  where num_tokens = 1 + (sequence_length ├Ќ num_numerical) + num_categorical
  = 1 + (10 ├Ќ 8) + 2 = 83
- FFN: O(num_layers ├Ќ num_tokens ├Ќ d_model ├Ќ d_ffn)
- Dominated by attention: O(L ├Ќ T┬▓ ├Ќ d) where T=83

­Ъје ADVANTAGES OVER DUAL-PATH ARCHITECTURES:

1. Richer Feature Interactions:
   - Categorical features can directly attend to specific timesteps
   - Example: "sector" token can learn which historical prices are most relevant
   - Numerical tokens can attend to categorical context

2. Simpler Architecture:
   - Single transformer instead of two separate paths
   - Fewer parameters and simpler training dynamics
   - No fusion layer needed

3. Flexible Attention Patterns:
   - Model learns optimal attention between all feature types
   - No artificial separation between categorical and numerical
   - Better for features with complex interdependencies

4. Graceful Feature Handling:
   - Works seamlessly when num_categorical=0 (numerical only)
   - Simply omits categorical tokens from sequence
   - No architectural changes needed

РџА USAGE NOTES:

1. When num_categorical = 0:
   - Only numerical tokens and CLS are used
   - Token sequence: [CLS, num_tokens]
   - Total tokens: 1 + (sequence_length ├Ќ num_numerical) = 81

2. Feature Preparation:
   - Numerical: normalized/scaled continuous values
   - Categorical: integer indices [0, cardinality-1]
   - Ensure cardinalities match actual unique values

3. Training Considerations:
   - Use learning rate warmup for stable training
   - Gradient clipping helps with long sequences
   - Dropout applied uniformly to all tokens

4. Memory vs. Dual-Path:
   - Unified: O(T┬▓) where T = all tokens = 83
   - Dual: O(T_cat┬▓) + O(T_num┬▓) where T_cat=3, T_num=11
   - Unified: 83┬▓ = 6,889 vs Dual: 3┬▓ + 11┬▓ = 9 + 121 = 130
   - Unified has higher memory cost but richer interactions

This unified architecture is optimal for scenarios where cross-modal feature interactions
are important, such as learning how specific stock symbols respond differently to the
same market conditions, or how sector membership influences price pattern interpretation.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, List, Dict, Any
import numpy as np

# Import shared base components
from .base.embeddings import CLSToken, NumericalTokenizer, CategoricalEmbedding
from .base.prediction_heads import MultiHorizonHead
from .base.model_interface import TransformerBasedModel


class FeatureTokenizer(nn.Module):
    """Converts tabular features into transformer tokens."""
    
    def __init__(self, num_numerical: int, cat_cardinalities: List[int], d_token: int):
        super().__init__()
        
        # Validation
        if num_numerical + len(cat_cardinalities) == 0:
            logging.error("Must have at least one feature")
            raise ValueError("Must have at least one feature")
        
        self.num_numerical = num_numerical
        self.cat_cardinalities = cat_cardinalities
        self.d_token = d_token
        
        # CLS token for prediction (using shared base component)
        # Learnable parameter that aggregates all feature information
        self.cls_token = CLSToken(d_token)

        # Numerical feature tokenization using shared base component
        # Each numerical feature gets its own weight vector and bias vector
        if num_numerical > 0:
            self.num_tokenizer = NumericalTokenizer(num_numerical, d_token)

        # Categorical feature embeddings using shared base component
        # Each categorical feature gets its own embedding table: vocab_size Рєњ d_token
        if cat_cardinalities:
            self.cat_embedding = CategoricalEmbedding(cat_cardinalities, d_token)
    
    def forward(self, x_num: Optional[torch.Tensor], x_cat: Optional[torch.Tensor]) -> torch.Tensor:
        """
        Convert raw features into tokens for transformer processing.

        Args:
            x_num: Numerical features [batch_size, num_numerical]
            x_cat: Categorical features [batch_size, num_categorical]

        Returns:
            tokens: [batch_size, 1 + num_features, d_token]
                   First token is CLS, followed by feature tokens
        """
        batch_size = x_num.shape[0] if x_num is not None else x_cat.shape[0]
        tokens = []

        # Process numerical features using shared base tokenizer
        if self.num_numerical > 0 and x_num is not None:
            num_tokens = self.num_tokenizer(x_num)  # [batch, num_features, d_token]
            tokens.append(num_tokens)

        # Process categorical features using shared base embedding
        if self.cat_cardinalities and x_cat is not None:
            cat_tokens = self.cat_embedding(x_cat)  # [batch, num_categorical, d_token]
            tokens.append(cat_tokens)

        # Combine all feature tokens along feature dimension
        if not tokens:
            raise ValueError("No valid input features provided")

        feature_tokens = torch.cat(tokens, dim=1)  # [batch, total_features, d_token]

        # Prepend CLS token for final prediction aggregation (using shared base component)
        # CLS token will collect information from all features via self-attention
        cls_tokens = self.cls_token.expand(batch_size)  # [batch, 1, d_token]
        return torch.cat([cls_tokens, feature_tokens], dim=1)    # [batch, 1+features, d_token]


class SequenceFeatureTokenizer(nn.Module):
    """Tokenizes sequences of tabular features for temporal FT-Transformer."""
    
    def __init__(self, num_numerical: int, cat_cardinalities: List[int], d_token: int, sequence_length: int):
        super().__init__()
        
        # Validation
        if num_numerical + len(cat_cardinalities) == 0:
            raise ValueError("Must have at least one feature")
        
        self.num_numerical = num_numerical
        self.cat_cardinalities = cat_cardinalities
        self.d_token = d_token
        self.sequence_length = sequence_length
        
        # CLS token for prediction (using shared base component)
        self.cls_token = CLSToken(d_token)

        # Numerical feature tokenization using shared base component (same for all timesteps)
        if num_numerical > 0:
            self.num_tokenizer = NumericalTokenizer(num_numerical, d_token)

        # Categorical feature embeddings using shared base component (same for all timesteps)
        if cat_cardinalities:
            self.cat_embedding = CategoricalEmbedding(cat_cardinalities, d_token)
        
        # Temporal positional encoding - learnable position embeddings for each timestep
        self.temporal_pos_embedding = nn.Parameter(torch.randn(sequence_length, d_token))
    
    def forward(self, x_seq: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x_seq: Sequential features [batch_size, sequence_length, num_features]
            
        Returns:
            tokens: [batch_size, 1 + sequence_length * tokens_per_timestep, d_token]
        """
        batch_size, seq_len, total_features = x_seq.shape
        
        # Split into numerical and categorical features
        if self.num_numerical > 0 and len(self.cat_cardinalities) > 0:
            x_num_seq = x_seq[:, :, :self.num_numerical]  # [batch, seq_len, num_numerical]  
            x_cat_seq = x_seq[:, :, self.num_numerical:]  # [batch, seq_len, num_categorical]
        elif self.num_numerical > 0:
            x_num_seq = x_seq
            x_cat_seq = None
        else:
            x_num_seq = None
            x_cat_seq = x_seq
            
        all_tokens = []
        
        # Process each timestep
        for t in range(seq_len):
            timestep_tokens = []
            
            # Process numerical features for this timestep (using shared base tokenizer)
            if self.num_numerical > 0 and x_num_seq is not None:
                x_num_t = x_num_seq[:, t, :]  # [batch, num_numerical]
                num_tokens = self.num_tokenizer(x_num_t)  # [batch, num_numerical, d_token]
                timestep_tokens.append(num_tokens)

            # Process categorical features for this timestep (using shared base embedding)
            if self.cat_cardinalities and x_cat_seq is not None:
                x_cat_t = x_cat_seq[:, t, :].long()  # [batch, num_categorical]
                cat_tokens = self.cat_embedding(x_cat_t)  # [batch, num_categorical, d_token]
                timestep_tokens.append(cat_tokens)
            
            # Combine tokens for this timestep
            if timestep_tokens:
                timestep_combined = torch.cat(timestep_tokens, dim=1)  # [batch, tokens_per_timestep, d_token]
                
                # Add temporal positional encoding to all tokens of this timestep
                timestep_combined = timestep_combined + self.temporal_pos_embedding[t].unsqueeze(0).unsqueeze(0)
                
                all_tokens.append(timestep_combined)
        
        # Concatenate all timesteps: [batch, seq_len * tokens_per_timestep, d_token]
        if all_tokens:
            sequence_tokens = torch.cat(all_tokens, dim=1)
        else:
            # Fallback (shouldn't happen with validation)
            device = self.cls_token.device
            sequence_tokens = torch.empty(batch_size, 0, self.d_token, device=device)
        
        # Prepend CLS token (using shared base component)
        cls_tokens = self.cls_token.expand(batch_size)
        return torch.cat([cls_tokens, sequence_tokens], dim=1)


class FTTransformer(nn.Module):
    """Feature Tokenizer Transformer for tabular data."""
    
    def __init__(
        self,
        num_numerical: int,
        cat_cardinalities: List[int],
        d_token: int = 192,
        n_layers: int = 3,
        n_heads: int = 8,
        d_ffn: Optional[int] = None,
        dropout: float = 0.1,
        activation: str = 'gelu'
    ):
        super().__init__()
        
        if d_ffn is None:
            d_ffn = 4 * d_token
            
        # Feature tokenizer
        self.tokenizer = FeatureTokenizer(num_numerical, cat_cardinalities, d_token)
        
        # Transformer encoder - stack of identical layers with self-attention + FFN
        # Each layer: MultiHeadAttention Рєњ Add&Norm Рєњ FeedForward Рєњ Add&Norm
        # norm_first=True uses pre-normalization for better gradient flow
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_token,          # Token dimension (input/output size)
            nhead=n_heads,            # Number of attention heads
            dim_feedforward=d_ffn,    # Hidden size in feed-forward network
            dropout=dropout,          # Dropout rate
            activation=activation,    # Activation function (GELU/ReLU)
            batch_first=True,         # Input format: [batch, seq, features]
            norm_first=True           # Pre-norm for better training stability
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)

        # Output dimension for downstream prediction head
        self.d_token = d_token
    
    def forward(self, x_num: Optional[torch.Tensor], x_cat: Optional[torch.Tensor]) -> torch.Tensor:
        """
        Args:
            x_num: Numerical features [batch_size, num_numerical]
            x_cat: Categorical features [batch_size, num_categorical]
            
        Returns:
            cls_output: [batch_size, d_token] - CLS token representation
        """
        # Step 1: Convert features to tokens
        # tokens: [batch_size, 1 + num_features, d_token]
        tokens = self.tokenizer(x_num, x_cat)

        # Step 2: Apply self-attention across all tokens
        # Each token can attend to every other token, learning feature interactions
        # transformer_out: [batch_size, 1 + num_features, d_token]
        transformer_out = self.transformer(tokens)

        # Step 3: Extract CLS token for prediction
        # CLS token (first token) has aggregated information from all features
        return transformer_out[:, 0, :]  # [batch_size, d_token]


class SequenceFTTransformer(nn.Module):
    """Feature Tokenizer Transformer for sequential tabular data."""
    
    def __init__(
        self,
        num_numerical: int,
        cat_cardinalities: List[int],
        sequence_length: int,
        d_token: int = 192,
        n_layers: int = 3,
        n_heads: int = 8,
        d_ffn: Optional[int] = None,
        dropout: float = 0.1,
        activation: str = 'gelu'
    ):
        super().__init__()
        
        if d_ffn is None:
            d_ffn = 4 * d_token
            
        # Sequence feature tokenizer
        self.tokenizer = SequenceFeatureTokenizer(num_numerical, cat_cardinalities, d_token, sequence_length)
        
        # Transformer encoder - same as before, but will process more tokens
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_token,
            nhead=n_heads,
            dim_feedforward=d_ffn,
            dropout=dropout,
            activation=activation,
            batch_first=True,
            norm_first=True  # Pre-norm for better training stability
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)
        
        # Output head will be added by wrapper based on task
        self.d_token = d_token
    
    def forward(self, x_seq: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x_seq: Sequential features [batch_size, sequence_length, num_features]
            
        Returns:
            cls_output: [batch_size, d_token] - CLS token representation
        """
        # Tokenize sequence features  
        tokens = self.tokenizer(x_seq)
        
        # Pass through transformer
        transformer_out = self.transformer(tokens)
        
        # Return CLS token output
        return transformer_out[:, 0, :]  # [batch_size, d_token]


class FTTransformerPredictor(nn.Module):
    """Complete FT-Transformer with prediction head."""

    def __init__(
        self,
        num_numerical: int,
        cat_cardinalities: List[int],
        n_classes: int = 1,
        prediction_horizons: int = 1,
        **transformer_kwargs
    ):
        super().__init__()

        self.ft_transformer = FTTransformer(num_numerical, cat_cardinalities, **transformer_kwargs)
        self.prediction_horizons = prediction_horizons

        # Prediction head using shared base component
        d_token = self.ft_transformer.d_token
        self.head = MultiHorizonHead(
            d_input=d_token,
            prediction_horizons=prediction_horizons,
            hidden_dim=None,  # Simple linear head
            dropout=transformer_kwargs.get('dropout', 0.1)
        )

        self.n_classes = n_classes
    
    def forward(self, x_num: Optional[torch.Tensor], x_cat: Optional[torch.Tensor]) -> torch.Tensor:
        """
        Args:
            x_num: Numerical features
            x_cat: Categorical features

        Returns:
            predictions: [batch_size, n_classes * prediction_horizons]
                       For regression: [batch_size, prediction_horizons]
        """
        # Get CLS token representation from transformer
        cls_output = self.ft_transformer(x_num, x_cat)  # [batch, d_token]

        # Apply prediction head (MultiHorizonHead handles reshaping automatically)
        return self.head(cls_output)


class SequenceFTTransformerPredictor(nn.Module):
    """Complete Sequential FT-Transformer with prediction head."""

    def __init__(
        self,
        num_numerical: int,
        cat_cardinalities: List[int],
        sequence_length: int,
        n_classes: int = 1,
        prediction_horizons: int = 1,
        **transformer_kwargs
    ):
        super().__init__()

        self.sequence_ft_transformer = SequenceFTTransformer(
            num_numerical, cat_cardinalities, sequence_length, **transformer_kwargs
        )
        self.prediction_horizons = prediction_horizons

        # Prediction head using shared base component
        d_token = self.sequence_ft_transformer.d_token
        self.head = MultiHorizonHead(
            d_input=d_token,
            prediction_horizons=prediction_horizons,
            hidden_dim=None,  # Simple linear head
            dropout=transformer_kwargs.get('dropout', 0.1)
        )

        self.n_classes = n_classes
    
    def forward(self, x_seq: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x_seq: Sequential features [batch_size, sequence_length, num_features]

        Returns:
            predictions: [batch_size, n_classes * prediction_horizons]
                       For regression: [batch_size, prediction_horizons]
        """
        cls_output = self.sequence_ft_transformer(x_seq)
        # MultiHorizonHead handles reshaping automatically
        return self.head(cls_output)


class FTTransformerCLSModel(TransformerBasedModel):
    """
    ­ЪДа FT-TRANSFORMER WITH CLS TOKEN FOR CATEGORICAL FEATURES (FT_TRANSFORMER_CLS)
    ==============================================================================

    This model extends the FT-Transformer architecture to handle STATIC categorical features
    alongside TIME-VARYING numerical features for time series forecasting.

    ­ЪЊі ARCHITECTURE OVERVIEW:
    РћїРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћљ
    Рћѓ  Numerical Sequences (3D) + Categorical Features (2D) Рєњ Unified Tokens Рєњ    Рћѓ
    Рћѓ  Transformer Рєњ CLS Token Рєњ Prediction                                       Рћѓ
    РћћРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћў

    ­Ъћё DATA FLOW WITH MATRIX DIMENSIONS:

    Example Configuration:
    - batch_size = 32
    - sequence_length = 10
    - num_numerical = 8 (price, volume, seasonal features, etc.)
    - num_categorical = 2 (symbol, sector)
    - cat_cardinalities = [100, 5] (100 symbols, 5 sectors)
    - d_model = 128 (embedding dimension)

    Step 1: Input Format
    РћїРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћљ
    Рћѓ x_num: [32, 10, 8]     # Numerical sequences (time-varying)                 Рћѓ
    Рћѓ                        # Shape: [batch, seq_len, num_numerical_features]    Рћѓ
    Рћѓ                                                                              Рћѓ
    Рћѓ x_cat: [32, 2]         # Categorical features (static per sequence)         Рћѓ
    Рћѓ                        # Shape: [batch, num_categorical_features]           Рћѓ
    Рћѓ                        # Values are integer indices (label encoded)         Рћѓ
    РћћРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћў

    Step 2: Categorical Embedding with Logarithmic Dimension Scaling
    РћїРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћљ
    Рћѓ EMBEDDING DIMENSION FORMULA:                                                Рћѓ
    Рћѓ   emb_dim = int(8 * log2(cardinality + 1))                                 Рћѓ
    Рћѓ   emb_dim = clamp(emb_dim, d_model/4, d_model)                             Рћѓ
    Рћѓ                                                                              Рћѓ
    Рћѓ RATIONALE:                                                                   Рћѓ
    Рћѓ   - Logarithmic scaling: Information-theoretic capacity                     Рћѓ
    Рћѓ   - Lower bound (d_model/4): Ensures minimum representational capacity     Рћѓ
    Рћѓ   - Upper bound (d_model): Prevents excessive dimensions                   Рћѓ
    Рћѓ                                                                              Рћѓ
    Рћѓ EXAMPLES (d_model=128):                                                      Рћѓ
    Рћѓ   cardinality=10   Рєњ emb_dim = int(8*log2(11)) = 27 Рєњ clamp Рєњ 32          Рћѓ
    Рћѓ   cardinality=100  Рєњ emb_dim = int(8*log2(101)) = 53 Рєњ clamp Рєњ 53         Рћѓ
    Рћѓ   cardinality=1000 Рєњ emb_dim = int(8*log2(1001)) = 79 Рєњ clamp Рєњ 79        Рћѓ
    Рћѓ   cardinality=10000 Рєњ emb_dim = int(8*log2(10001)) = 106 Рєњ clamp Рєњ 106    Рћѓ
    Рћѓ                                                                              Рћѓ
    Рћѓ CATEGORICAL EMBEDDING:                                                       Рћѓ
    Рћѓ   symbol_embedding: Embedding(100, 53)   # 100 symbols Рєњ 53 dims           Рћѓ
    Рћѓ   sector_embedding: Embedding(5, 32)     # 5 sectors Рєњ 32 dims             Рћѓ
    Рћѓ                                                                              Рћѓ
    Рћѓ   x_cat: [32, 2] (integer indices)                                          Рћѓ
    Рћѓ   Рєњ symbol_emb: [32, 53]                                                    Рћѓ
    Рћѓ   Рєњ sector_emb: [32, 32]                                                    Рћѓ
    Рћѓ                                                                              Рћѓ
    Рћѓ PROJECT TO d_model:                                                          Рћѓ
    Рћѓ   symbol_proj: Linear(53, 128) Рєњ [32, 128]                                 Рћѓ
    Рћѓ   sector_proj: Linear(32, 128) Рєњ [32, 128]                                 Рћѓ
    Рћѓ   cat_tokens: [32, 2, 128]  # Stacked categorical tokens                   Рћѓ
    РћћРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћў

    Step 3: Numerical Tokenization
    РћїРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћљ
    Рћѓ For each numerical feature at each timestep:                                Рћѓ
    Рћѓ   token_ij = x_ij * W_j + b_j                                              Рћѓ
    Рћѓ                                                                              Рћѓ
    Рћѓ num_tokenizer: NumericalTokenizer(8, 128)                                   Рћѓ
    Рћѓ                                                                              Рћѓ
    Рћѓ Process each timestep:                                                       Рћѓ
    Рћѓ   for t in range(10):                                                        Рћѓ
    Рћѓ     x_num_t = x_num[:, t, :]  # [32, 8]                                    Рћѓ
    Рћѓ     tokens_t = num_tokenizer(x_num_t)  # [32, 8, 128]                      Рћѓ
    Рћѓ                                                                              Рћѓ
    Рћѓ Concatenate all timesteps:                                                  Рћѓ
    Рћѓ   num_tokens: [32, 80, 128]  # 10 timesteps ├Ќ 8 features                   Рћѓ
    РћћРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћў

    Step 4: Token Assembly (UNIFIED PROCESSING)
    РћїРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћљ
    Рћѓ Token Order: [CLS, numerical_t0...t9, categorical]                          Рћѓ
    Рћѓ                                                                              Рћѓ
    Рћѓ 1. CLS Token: [32, 1, 128]  # Learnable aggregation token                  Рћѓ
    Рћѓ 2. Numerical Tokens: [32, 80, 128]  # 10 timesteps ├Ќ 8 features            Рћѓ
    Рћѓ 3. Categorical Tokens: [32, 2, 128]  # 2 categorical features              Рћѓ
    Рћѓ                                                                              Рћѓ
    Рћѓ Concatenate along sequence dimension:                                       Рћѓ
    Рћѓ   all_tokens = cat([CLS, num_tokens, cat_tokens], dim=1)                   Рћѓ
    Рћѓ   Рєњ [32, 83, 128]  # 1 CLS + 80 numerical + 2 categorical                 Рћѓ
    РћћРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћў

    Step 5: Transformer Processing
    РћїРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћљ
    Рћѓ Multi-Head Self-Attention (num_heads=8):                                    Рћѓ
    Рћѓ   d_head = 128 / 8 = 16                                                     Рћѓ
    Рћѓ                                                                              Рћѓ
    Рћѓ   Q, K, V = all_tokens @ W_q, W_k, W_v  # [32, 83, 128]                   Рћѓ
    Рћѓ   Reshape: [32, 8, 83, 16]  # [batch, heads, seq, d_head]                 Рћѓ
    Рћѓ                                                                              Рћѓ
    Рћѓ   Attention(Q,K,V) = softmax(QK^T/Рѕџ16)V                                     Рћѓ
    Рћѓ   scores: [32, 8, 83, 83]  # All tokens attend to all tokens              Рћѓ
    Рћѓ   output: [32, 83, 128]                                                     Рћѓ
    Рћѓ                                                                              Рћѓ
    Рћѓ Feed-Forward Network:                                                        Рћѓ
    Рћѓ   FFN(x) = Linear(GELU(Linear(x, 512)), 128)                               Рћѓ
    Рћѓ   output: [32, 83, 128]                                                     Рћѓ
    Рћѓ                                                                              Рћѓ
    Рћѓ Repeat for num_layers (typically 3-6)                                       Рћѓ
    РћћРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћў

    Step 6: Prediction from CLS Token
    РћїРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћљ
    Рћѓ Extract CLS token (first token):                                            Рћѓ
    Рћѓ   cls_output = transformer_output[:, 0, :]  # [32, 128]                    Рћѓ
    Рћѓ                                                                              Рћѓ
    Рћѓ Prediction Head:                                                             Рћѓ
    Рћѓ   predictions = Linear(cls_output, output_dim)  # [32, output_dim]         Рћѓ
    Рћѓ                                                                              Рћѓ
    Рћѓ For multi-horizon forecasting (prediction_horizon=3):                       Рћѓ
    Рћѓ   output_dim = 3                                                             Рћѓ
    Рћѓ   predictions: [32, 3]  # Predictions for horizons 1, 2, 3                 Рћѓ
    РћћРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћђРћў

    ­Ъј» KEY DESIGN DECISIONS:

    1. **Categorical Embedding Dimensions**: Logarithmic scaling with bounds
       - Formula: emb_dim = clamp(int(8 * log2(cardinality + 1)), d_model/4, d_model)
       - Balances capacity with efficiency
       - Information-theoretic foundation

    2. **Token Order**: CLS Рєњ Numerical (time-ordered) Рєњ Categorical
       - CLS token aggregates all information
       - Numerical tokens preserve temporal structure
       - Categorical tokens provide static context

    3. **Unified Processing**: Single transformer processes all tokens
       - Attention mechanism learns optimal feature interactions
       - Categorical features attend to all timesteps
       - Numerical features attend to categorical context

    4. **Static vs Dynamic Features**:
       - Numerical: 3D sequences (time-varying)
       - Categorical: 2D features (static per sequence)
       - Categorical extracted from LAST timestep of each sequence

    ­Ъџђ ADVANTAGES:

    1. **Flexible Feature Handling**: Handles heterogeneous feature types
    2. **Temporal + Static Context**: Combines time-varying and static information
    3. **Automatic Interaction Discovery**: Attention learns complex relationships
    4. **Scalable**: Logarithmic embedding dimensions scale efficiently

    Рџа№ИЈ IMPORTANT NOTES:

    1. **Unseen Categories**: Will throw an error (by design)
       - Categorical features must be label encoded during training
       - New categories at inference time are not supported

    2. **Input Format**:
       - x_num: Numerical sequences [batch, seq_len, num_numerical]
       - x_cat: Categorical indices [batch, num_categorical] (dtype: long)

    3. **Categorical Columns**: Must be specified and encoded in predictor.py
       - Encoding: sklearn.preprocessing.LabelEncoder
       - Sequence creation: Extract from last timestep

    This implementation provides a robust foundation for time series forecasting with
    mixed feature types, combining temporal dynamics with static categorical context.
    """

    def __init__(
        self,
        sequence_length: int,
        num_numerical: int,
        num_categorical: int,
        cat_cardinalities: List[int],
        output_dim: int,
        d_model: int = 128,
        num_heads: int = 8,
        num_layers: int = 3,
        dropout: float = 0.1,
        activation: str = 'gelu'
    ):
        """
        Initialize FT-Transformer with CLS token for categorical features.

        Args:
            sequence_length: Length of input sequences (lookback window)
            num_numerical: Number of numerical features per timestep
            num_categorical: Number of categorical features
            cat_cardinalities: List of cardinalities for each categorical feature
            output_dim: Output dimension (num_targets * prediction_horizon)
            d_model: Embedding dimension
            num_heads: Number of attention heads
            num_layers: Number of transformer layers
            dropout: Dropout rate
            activation: Activation function ('relu' or 'gelu')
        """
        super().__init__(d_model=d_model, num_heads=num_heads, num_layers=num_layers)

        self.sequence_length = sequence_length
        self.num_numerical = num_numerical
        self.num_categorical = num_categorical
        self.cat_cardinalities = cat_cardinalities
        self.output_dim = output_dim
        self.dropout_rate = dropout
        self.activation_name = activation

        # Validate inputs
        if num_categorical > 0 and len(cat_cardinalities) != num_categorical:
            raise ValueError(f"cat_cardinalities length ({len(cat_cardinalities)}) must match num_categorical ({num_categorical})")

        # CLS token for prediction
        self.cls_token = CLSToken(d_model)

        # Numerical tokenizer
        if num_numerical > 0:
            self.num_tokenizer = NumericalTokenizer(num_numerical, d_model)

        # Categorical embeddings with logarithmic dimension scaling
        if num_categorical > 0:
            import math

            self.cat_embeddings = nn.ModuleList()
            self.cat_projections = nn.ModuleList()

            for cardinality in cat_cardinalities:
                # Calculate embedding dimension using logarithmic scaling
                emb_dim = int(8 * math.log2(cardinality + 1))
                # Clamp to bounds [d_model/4, d_model]
                min_dim = d_model // 4
                max_dim = d_model
                emb_dim = max(min_dim, min(max_dim, emb_dim))

                # Create embedding layer
                embedding = nn.Embedding(cardinality, emb_dim)
                self.cat_embeddings.append(embedding)

                # Project to d_model if needed
                if emb_dim != d_model:
                    projection = nn.Linear(emb_dim, d_model)
                    self.cat_projections.append(projection)
                else:
                    self.cat_projections.append(nn.Identity())

        # Positional encoding for temporal sequences (optional, can help)
        self.temporal_pos_encoding = nn.Parameter(torch.randn(1, sequence_length, d_model) * 0.02)

        # Transformer encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=num_heads,
            dim_feedforward=4 * d_model,
            dropout=dropout,
            activation=activation,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

        # Prediction head
        self.head = MultiHorizonHead(
            d_input=d_model,
            prediction_horizons=output_dim,
            hidden_dim=None,
            dropout=dropout
        )

    def forward(self, x_num: torch.Tensor, x_cat: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        Forward pass with separate numerical and categorical inputs.

        Args:
            x_num: Numerical sequences [batch_size, sequence_length, num_numerical]
            x_cat: Categorical features [batch_size, num_categorical] (integer indices)

        Returns:
            predictions: [batch_size, output_dim]
        """
        batch_size = x_num.shape[0]

        # Step 1: Create CLS token
        cls_tokens = self.cls_token(batch_size)  # [batch, 1, d_model]

        # Step 2: Tokenize numerical sequences
        # Process each timestep independently
        num_tokens_list = []
        for t in range(self.sequence_length):
            x_num_t = x_num[:, t, :]  # [batch, num_numerical]
            tokens_t = self.num_tokenizer(x_num_t)  # [batch, num_numerical, d_model]
            # Add temporal positional encoding
            tokens_t = tokens_t + self.temporal_pos_encoding[:, t:t+1, :].expand(-1, self.num_numerical, -1)
            num_tokens_list.append(tokens_t)

        # Concatenate all timesteps: [batch, seq_len * num_numerical, d_model]
        num_tokens = torch.cat(num_tokens_list, dim=1)

        # Step 3: Process categorical features (if present)
        if x_cat is not None and self.num_categorical > 0:
            cat_tokens_list = []
            for i in range(self.num_categorical):
                cat_indices = x_cat[:, i]  # [batch]
                emb = self.cat_embeddings[i](cat_indices)  # [batch, emb_dim]
                projected = self.cat_projections[i](emb)  # [batch, d_model]
                cat_tokens_list.append(projected.unsqueeze(1))  # [batch, 1, d_model]

            cat_tokens = torch.cat(cat_tokens_list, dim=1)  # [batch, num_categorical, d_model]

            # Step 4: Concatenate all tokens: [CLS, numerical, categorical]
            all_tokens = torch.cat([cls_tokens, num_tokens, cat_tokens], dim=1)
        else:
            # No categorical features
            all_tokens = torch.cat([cls_tokens, num_tokens], dim=1)

        # Step 5: Transformer processing
        transformer_output = self.transformer(all_tokens)  # [batch, num_tokens, d_model]

        # Step 6: Extract CLS token and predict
        cls_output = transformer_output[:, 0, :]  # [batch, d_model]
        predictions = self.head(cls_output)  # [batch, output_dim]

        return predictions

    def get_model_config(self) -> Dict[str, Any]:
        """Get the current configuration of the model."""
        return {
            'model_type': 'ft_transformer_cls',
            'd_model': self.d_model,
            'num_heads': self.num_heads,
            'num_layers': self.num_layers,
            'dropout': self.dropout_rate,
            'activation': self.activation_name,
            'sequence_length': self.sequence_length,
            'num_numerical': self.num_numerical,
            'num_categorical': self.num_categorical,
            'cat_cardinalities': self.cat_cardinalities,
            'output_dim': self.output_dim
        }

    def get_attention_weights(self) -> Optional[torch.Tensor]:
        """
        Get attention weights from the last forward pass.

        Returns:
            Attention weights tensor or None if not available.
        """
        return None  # Not implemented for CLS models
